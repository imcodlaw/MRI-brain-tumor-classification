{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n#import numpy as np # linear algebra\n#import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from IPython.display import clear_output\n!pip install imutils\nclear_output()\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os, keras\nimport shutil\nimport cv2\n\nimport imutils\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom keras.preprocessing.image import load_img, img_to_array, array_to_img\nfrom tqdm.notebook import tqdm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nimport glob\nimport tensorflow as tf\n\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, Flatten, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D\nfrom tensorflow.keras.layers import Concatenate\nfrom tensorflow.keras.optimizers import Adam, SGD","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.patches as mpatches\nfrom matplotlib.lines import Line2D\nfrom matplotlib.patches import Rectangle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy.random import seed\nseed(88)\nfrom tensorflow.random import set_seed\nset_seed(88)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def crop_imgs(set_name, add_pixels_value=0):\n    \"\"\"\n    Finds the extreme points on the image and crops the rectangular out of them\n    \"\"\"\n    set_new = []\n    for img in set_name:\n        img = cv2.imread(img) #original syntax\n        #image = load_img(img,\n                         #color_mode = 'grayscale')\n        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) #original syntax\n        #gray = np.array(image)\n        gray = cv2.GaussianBlur(gray, (5, 5), 0)\n\n        # threshold the image, then perform a series of erosions +\n        # dilations to remove any small regions of noise\n        thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n        thresh = cv2.erode(thresh, None, iterations=2)\n        thresh = cv2.dilate(thresh, None, iterations=2)\n\n        # find contours in thresholded image, then grab the largest one\n        cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        cnts = imutils.grab_contours(cnts)\n        c = max(cnts, key=cv2.contourArea)\n\n        # find the extreme points\n        extLeft = tuple(c[c[:, :, 0].argmin()][0])\n        extRight = tuple(c[c[:, :, 0].argmax()][0])\n        extTop = tuple(c[c[:, :, 1].argmin()][0])\n        extBot = tuple(c[c[:, :, 1].argmax()][0])\n\n        ADD_PIXELS = add_pixels_value\n        new_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()\n        set_new.append(new_img)\n\n    return np.array(set_new)\n\ndef crop_imgs_array(set_name, add_pixels_value=0):\n    \"\"\"\n    Finds the extreme points on the image and crops the rectangular out of them\n    \"\"\"\n    set_new = []\n    for img in set_name:\n        gray = cv2.GaussianBlur(img, (5, 5), 0)\n\n        # threshold the image, then perform a series of erosions +\n        # dilations to remove any small regions of noise\n        thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n        thresh = cv2.erode(thresh, None, iterations=2)\n        thresh = cv2.dilate(thresh, None, iterations=2)\n\n        # find contours in thresholded image, then grab the largest one\n        cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        cnts = imutils.grab_contours(cnts)\n        c = max(cnts, key=cv2.contourArea)\n\n        # find the extreme points\n        extLeft = tuple(c[c[:, :, 0].argmin()][0])\n        extRight = tuple(c[c[:, :, 0].argmax()][0])\n        extTop = tuple(c[c[:, :, 1].argmin()][0])\n        extBot = tuple(c[c[:, :, 1].argmax()][0])\n\n        ADD_PIXELS = add_pixels_value\n        new_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()\n        set_new.append(new_img)\n\n    return np.array(set_new)\n\ndef gray_scale_img(path) :\n    for sub_dir in tqdm(os.listdir(path)) :\n        for file in os.listdir(path + '/' + sub_dir) :\n            src = load_img(path + '/' + sub_dir + '/' + file,\n                           color_mode = 'grayscale')\n            #src = cv2.imread(path + '/' + sub_dir + '/' + file)\n            output = np.array(src)\n            cv2.imwrite(path + '/' + sub_dir + '/' + file, output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#os.listdir('./VAL')\ndef resize_img_path(path, dsize) :\n    for sub_dir in tqdm(os.listdir(path)) :\n        for file in os.listdir(path + '/' + sub_dir) :\n            src = cv2.imread(path + '/' + sub_dir + '/' + file)\n            output = cv2.resize(src, dsize)\n            cv2.imwrite(path + '/' + sub_dir + '/' + file,output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#os.listdir('./VAL')\ndef resize_img(path, dsize) :\n    result = []\n    #for sub_dir in tqdm(os.listdir(path)) :\n    for file in os.listdir(path) :\n        src = cv2.imread(path + '/' + file)\n        output = cv2.resize(src, dsize)\n        #cv2.imwrite(path + '/' + sub_dir + '/' + file,output)\n        result.append(output)\n    return np.array(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def resize_img_array(data, dsize) :\n    result = []\n    #for sub_dir in tqdm(os.listdir(path)) :\n    for img in data :\n        output = cv2.resize(img, dsize)\n        result.append(output)\n    return np.array(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prob_converter(data) :\n    dim = data[0].shape[0]\n    result = []\n    prob = []\n    for i in range(dim) :\n        res = []\n        res.append(np.mean((data[0][i][0], data[1][i][0], data[2][i][0])))\n        res.append(np.mean((data[0][i][1], data[1][i][1], data[2][i][1])))\n        res.append(np.mean((data[0][i][2], data[1][i][2], data[2][i][2])))\n        res.append(np.mean((data[0][i][3], data[1][i][3], data[2][i][3])))\n        prob.append(res)\n    prob = np.array(prob)\n    \n    for i in prob :\n        result.append(np.where(i == i.max())[0][0])\n    return result\n\ndef prob_average(data) :\n    dim = data[0].shape[0]\n    result = []\n    prob = []\n    for i in range(dim) :\n        res = []\n        res.append(np.mean((data[0][i][0], data[1][i][0], data[2][i][0])))\n        res.append(np.mean((data[0][i][1], data[1][i][1], data[2][i][1])))\n        res.append(np.mean((data[0][i][2], data[1][i][2], data[2][i][2])))\n        res.append(np.mean((data[0][i][3], data[1][i][3], data[2][i][3])))\n        prob.append(res)\n    prob = np.array(prob)\n    \n    return prob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IMG_PATH = '../input/brain-mri-os'\n#for Class in os.listdir(IMG_PATH) :\n#    print(Class)\nprint('glioma_tumor : ' + str(len([file for file in os.listdir('../input/brain-mri-os/Training fix/glioma_tumor_fix')])))\nprint('meningioma_tumor : ' + str(len([file for file in os.listdir('../input/brain-mri-os/Training fix/meningioma_tumor_fix')])))\nprint('hipofisis tumor : ' + str(len([file for file in os.listdir('../input/brain-mri-os/Training fix/pituitary_tumor_fix')])))\nprint('no tumor : ' + str(len([file for file in os.listdir('../input/brain-mri-os/Training fix/no_tumor_fix')])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IMG_PATH = '../input/brain-mri-os/Training fix/'\nimg_file = []\nlabel = []\nsize = []\npath_img = []\nfor path in os.listdir(IMG_PATH) :    \n    for item in os.listdir(IMG_PATH + str(path)) :\n        img_file.append(item)\n        label.append(str(path))\n        size.append(Image.open(IMG_PATH +str(path)+'/'+str(item)).size)\n        path_img.append(IMG_PATH +str(path)+'/'+str(item))\ntrain_df = pd.DataFrame({'file' : img_file, 'path' : path_img, 'label' : label, 'size' : size})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x, val_x, train_y, val_y = train_test_split(train_df[['file', 'path', 'size']],\n                                                  train_df[['label']],\n                                                  train_size = .8,\n                                                  stratify = train_df['label'],\n                                                  random_state = 2)\n\ntrain_x2, val_x2, train_y2, val_y2 = train_test_split(train_df[['file', 'path', 'size']],\n                                                  train_df[['label']],\n                                                  train_size = .8,\n                                                  stratify = train_df['label'],\n                                                  random_state = 3)\n\ntrain_x3, val_x3, train_y3, val_y3 = train_test_split(train_df[['file', 'path', 'size']],\n                                                  train_df[['label']],\n                                                  train_size = .8,\n                                                  stratify = train_df['label'],\n                                                  random_state = 4)\n\ntrain_x4, val_x4, train_y4, val_y4 = train_test_split(train_df[['file', 'path', 'size']],\n                                                  train_df[['label']],\n                                                  train_size = .8,\n                                                  stratify = train_df['label'],\n                                                  random_state = 5)\n\ntrain_x5, val_x5, train_y5, val_y5 = train_test_split(train_df[['file', 'path', 'size']],\n                                                  train_df[['label']],\n                                                  train_size = .8,\n                                                  stratify = train_df['label'],\n                                                  random_state = 6)\ntrain_set = pd.concat([train_x.reset_index(drop = True),\n                       train_y.reset_index(drop = True)],\n                      axis = 1)\nval_set = pd.concat([val_x.reset_index(drop = True),\n                     val_y.reset_index(drop = True)],\n                      axis = 1)\n\n\"\"\"train_set2 = pd.concat([train_x2.reset_index(drop = True),\n                       train_y2.reset_index(drop = True)],\n                      axis = 1)\nval_set2 = pd.concat([val_x2.reset_index(drop = True),\n                     val_y2.reset_index(drop = True)],\n                      axis = 1)\n\ntrain_set3 = pd.concat([train_x3.reset_index(drop = True),\n                       train_y3.reset_index(drop = True)],\n                      axis = 1)\nval_set3 = pd.concat([val_x3.reset_index(drop = True),\n                     val_y3.reset_index(drop = True)],\n                      axis = 1)\n\ntrain_set4 = pd.concat([train_x4.reset_index(drop = True),\n                       train_y4.reset_index(drop = True)],\n                      axis = 1)\nval_set4 = pd.concat([val_x4.reset_index(drop = True),\n                     val_y4.reset_index(drop = True)],\n                      axis = 1)\n\ntrain_set5 = pd.concat([train_x5.reset_index(drop = True),\n                       train_y5.reset_index(drop = True)],\n                      axis = 1)\nval_set5 = pd.concat([val_x5.reset_index(drop = True),\n                     val_y5.reset_index(drop = True)],\n                      axis = 1)\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!apt-get install tree\n## create new folders\n!mkdir TRAIN TEST VAL TRAIN/GLIOMA_TUMOR_FIX TRAIN/MENINGIOMA_TUMOR_FIX TRAIN/NO_TUMOR_FIX TRAIN/PITUITARY_TUMOR_FIX VAL/GLIOMA_TUMOR_FIX VAL/MENINGIOMA_TUMOR_FIX VAL/NO_TUMOR_FIX VAL/PITUITARY_TUMOR_FIX\n!tree -d\n\n\n#IMG_PATH = '../input/brain-tumor-classification-mri/Training/'\n# split the data by train/val/test\n#for CLASS in os.listdir(IMG_PATH):\n    #if not CLASS.startswith('.'):\n#    IMG_NUM = len(os.listdir(IMG_PATH + CLASS))\n#    for (n, FILE_NAME) in enumerate(os.listdir(IMG_PATH + CLASS)):\n#        print(FILE_NAME)\n        \n\"\"\"IMG_PATH = '../input/brain-mri-os/Training fix/'\n# split the data by train/val/test\nfor CLASS in os.listdir(IMG_PATH):\n    #for (n, FILE_NAME) in enumerate(os.listdir(IMG_PATH + CLASS)):\n    for FILE_NAME in os.listdir(IMG_PATH+CLASS) :\n        img = IMG_PATH + CLASS + '/' + FILE_NAME\n        if FILE_NAME in list(train_set['file']) :\n            shutil.copy(img, 'TRAIN/' + CLASS.upper() + '/')\n        else :\n            shutil.copy(img, 'VAL/'+ CLASS.upper() + '/')\"\"\"\n            \n\n# train no tumor path\nfor img in train_set[train_set['label'] == 'no_tumor_fix']['path'] :\n    shutil.copy(img, 'TRAIN/NO_TUMOR_FIX/')\n\nfor img in train_set[train_set['label'] == 'meningioma_tumor_fix']['path'] :\n    shutil.copy(img, 'TRAIN/MENINGIOMA_TUMOR_FIX/')\n    \nfor img in train_set[train_set['label'] == 'glioma_tumor_fix']['path'] :\n    shutil.copy(img, 'TRAIN/GLIOMA_TUMOR_FIX/')\n\nfor img in train_set[train_set['label'] == 'pituitary_tumor_fix']['path'] :\n    shutil.copy(img, 'TRAIN/PITUITARY_TUMOR_FIX/')\n    \nfor img in val_set[val_set['label'] == 'no_tumor_fix']['path'] :\n    shutil.copy(img, 'VAL/NO_TUMOR_FIX/')\n\nfor img in val_set[val_set['label'] == 'meningioma_tumor_fix']['path'] :\n    shutil.copy(img, 'VAL/MENINGIOMA_TUMOR_FIX/')\n    \nfor img in val_set[val_set['label'] == 'glioma_tumor_fix']['path'] :\n    shutil.copy(img, 'VAL/GLIOMA_TUMOR_FIX/')\n\nfor img in val_set[val_set['label'] == 'pituitary_tumor_fix']['path'] :\n    shutil.copy(img, 'VAL/PITUITARY_TUMOR_FIX/')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### train val label distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style('darkgrid')\nfig, ax = plt.subplots(1, 2, figsize = [15, 5])\nsns.countplot(train_set[['label']].sort_values(by = 'label', ascending = True)['label'], ax = ax[0])\nsns.countplot(val_set[['label']].sort_values(by = 'label', ascending = True)['label'], ax = ax[1])\nax[0].set_xticklabels(['glioma', 'meningioma', 'no_tumor', 'pituitary (hipofisis)'])\nax[1].set_xticklabels(['glioma', 'meningioma', 'no_tumor', 'pituitary (hipofisis)'])\nax[0].set_title('Training Set Label Distribution')\nax[1].set_title('Validation Set Label Distribution')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### showing sample images"},{"metadata":{"trusted":true},"cell_type":"code","source":"r = 3\nc = 3\ni = 1\nplt.figure(figsize = [15, 15])\nplt.suptitle('Pituitary Tumor', y = .92, size = 25)\nfor path in train_set[train_set['label'] == 'pituitary_tumor_fix'].sample(9, random_state = 2)['path'] :\n    plt.subplot(r,c,i)\n    img = load_img(path)\n    plt.imshow(img)\n    plt.xticks([])\n    plt.yticks([])\n    i += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_notumor_path = glob.glob('./TRAIN/NO_TUMOR_FIX/*.*')\ntrain_meningioma_path = glob.glob('./TRAIN/MENINGIOMA_TUMOR_FIX/*.*')\ntrain_glioma_path = glob.glob('./TRAIN/GLIOMA_TUMOR_FIX/*.*')\ntrain_pituitary_path = glob.glob('./TRAIN/PITUITARY_TUMOR_FIX/*.*')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_notumor_path = glob.glob('./VAL/NO_TUMOR_FIX/*.*')\nval_meningioma_path = glob.glob('./VAL/MENINGIOMA_TUMOR_FIX/*.*')\nval_glioma_path = glob.glob('./VAL/GLIOMA_TUMOR_FIX/*.*')\nval_pituitary_path = glob.glob('./VAL/PITUITARY_TUMOR_FIX/*.*')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"sorting the path just like train_set and val_set"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_notumor_path2 = list(train_set[train_set['label'] == 'no_tumor_fix']['path'])\ntrain_meningioma_path2 = list(train_set[train_set['label'] == 'meningioma_tumor_fix']['path'])\ntrain_glioma_path2 = list(train_set[train_set['label'] == 'glioma_tumor_fix']['path'])\ntrain_pituitary_path2 = list(train_set[train_set['label'] == 'pituitary_tumor_fix']['path'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_notumor_path2 = list(val_set[val_set['label'] == 'no_tumor_fix']['path'])\nval_meningioma_path2 = list(val_set[val_set['label'] == 'meningioma_tumor_fix']['path'])\nval_glioma_path2 = list(val_set[val_set['label'] == 'glioma_tumor_fix']['path'])\nval_pituitary_path2 = list(val_set[val_set['label'] == 'pituitary_tumor_fix']['path'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_glioma\nnp.random.seed(88)\nrandom_index = list(np.random.randint(1, 100, 5))\nrandom_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def grayscale_plot(list_path, title) :\n    n_img = len(list_path)\n    fig = plt.figure(figsize = [15, (1*n_img)])\n    i = 1\n    plt.title(title, size = 20)\n    plt.xticks([])\n    plt.yticks([])\n    sns.set_style('dark')\n    for path in list_path :\n        img = cv2.imread(path)\n        ax = fig.add_subplot(2, n_img, i)\n        plt.imshow(cv2.cvtColor(img, cv2.COLOR_RGB2GRAY))\n        plt.xticks([])\n        plt.yticks([])\n        #i += 1\n        \n        ax = fig.add_subplot(2, n_img, i+5)\n        plt.imshow(img)\n        plt.xticks([])\n        plt.yticks([])\n        i += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grayscale_plot(np.array(train_pituitary_path)[random_index], 'Hipofisis\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#list(train_df['size'].value_counts())\nindex = [str(i) for i in train_df['size'].value_counts()[:10].index]\nindex += ['other']\nval = list(train_df['size'].value_counts()[:10])\nval += [sum(train_df['size'].value_counts()[10:])]\nsns.set_style('white')\nplt.figure(figsize = [12, 5])\nsns.barplot(y = val,\n            x = index,\n            palette = 'viridis_r')\nplt.title('Barplot Banyaknya Gambar dengan Ukuran Tertentu')\nfor index, value in enumerate(val) :\n    plt.text(index-.15, value, value)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Syntax Resize"},{"metadata":{"trusted":true},"cell_type":"code","source":"resize_img_path('./VAL', (224, 224))\nresize_img_path('./TRAIN', (224, 224))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tumor_encode_dict = {'no_tumor' : 0,\n                     'meningioma' : 1,\n                     'glioma' : 2,\n                     'pituitary' : 3}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def crop_plot(nocrop_path, crop_array, title) :\n    n_img = len(nocrop_path)\n    fig = plt.figure(figsize = [15, (1*n_img)])\n    i = 1\n    plt.title(title, size = 20)\n    plt.xticks([])\n    plt.yticks([])\n    sns.set_style('dark')\n    for nocrop, crop in zip(nocrop_path, crop_array) :\n        img = cv2.imread(nocrop)        \n        ax = fig.add_subplot(2, n_img, i)\n        plt.imshow(img)\n        plt.xticks([])\n        plt.yticks([])\n        #i += 1\n        \n        ax = fig.add_subplot(2, n_img, i+5)\n        plt.imshow(crop)\n        plt.xticks([])\n        plt.yticks([])\n\n        i += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"crop_train_notumor = crop_imgs(train_notumor_path)\ncrop_train_meningioma = crop_imgs(train_meningioma_path)\ncrop_train_glioma = crop_imgs(train_glioma_path)\ncrop_train_pituitary = crop_imgs(train_pituitary_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cv2.imread(train_notumor_path[0], cv2.CV_LOAD_IMAGE_GRAYSCALE)\n#cv2.CV_LOAD_IMAGE_GRAYSCALE(train_notumor_path[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_notumor[0]\n\"\"\"X_train = []\ny_train = []\nfor notumor in train_notumor_path :\n    image = tf.keras.preprocessing.image.load_img(notumor,\n                                                  color_mode = 'grayscale')\n    image = np.array(image)\n    X_train.append(image)\n    y_train.append(0)\n    \nfor meningioma in train_meningioma_path :\n    image = tf.keras.preprocessing.image.load_img(meningioma,\n                                                  color_mode = 'grayscale')\n    image = np.array(image)\n    X_train.append(image)\n    y_train.append(1)\n    \nfor glioma in train_glioma_path :\n    image = tf.keras.preprocessing.image.load_img(glioma,\n                                                  color_mode = 'grayscale')\n    image = np.array(image)\n    X_train.append(image)\n    y_train.append(2)\n    \nfor pituitary in train_pituitary_path :\n    image = tf.keras.preprocessing.image.load_img(pituitary,\n                                                  color_mode = 'grayscale')\n    image = np.array(image)\n    X_train.append(image)\n    y_train.append(3)\n    \nX_train = np.array(X_train)\ny_train = np.array(y_train)\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_notumor[0]\nX_train = []\nX_train_name = []\ny_train = []\nfor notumor in train_notumor_path2 :\n    dir_ = './TRAIN/NO_TUMOR_FIX/' + notumor.split('_fix/', 1)[1]\n    image = tf.keras.preprocessing.image.load_img(dir_,\n                                                  color_mode = 'grayscale')\n    image = np.array(image)\n    X_train_name.append(dir_)\n    X_train.append(image)\n    y_train.append(0)\n    \nfor meningioma in train_meningioma_path2 :\n    dir_ = './TRAIN/MENINGIOMA_TUMOR_FIX/' + meningioma.split('_fix/', 1)[1]\n    image = tf.keras.preprocessing.image.load_img(dir_,\n                                                  color_mode = 'grayscale')\n    image = np.array(image)\n    X_train_name.append(dir_)\n    X_train.append(image)\n    y_train.append(1)\n    \nfor glioma in train_glioma_path2 :\n    dir_ = './TRAIN/GLIOMA_TUMOR_FIX/' + glioma.split('_fix/', 1)[1]\n    image = tf.keras.preprocessing.image.load_img(dir_,\n                                                  color_mode = 'grayscale')\n    image = np.array(image)\n    X_train_name.append(dir_)\n    X_train.append(image)\n    y_train.append(2)\n    \nfor pituitary in train_pituitary_path2 :\n    dir_ = './TRAIN/PITUITARY_TUMOR_FIX/' + pituitary.split('_fix/', 1)[1]\n    image = tf.keras.preprocessing.image.load_img(dir_,\n                                                  color_mode = 'grayscale')\n    image = np.array(image)\n    X_train_name.append(dir_)\n    X_train.append(image)\n    y_train.append(3)\n    \nX_train = np.array(X_train)\ny_train = np.array(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"crop_val_notumor = crop_imgs(val_notumor_path)\ncrop_val_meningioma = crop_imgs(val_meningioma_path)\ncrop_val_glioma = crop_imgs(val_glioma_path)\ncrop_val_pituitary = crop_imgs(val_pituitary_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#val_notumor[0]\n\"\"\"X_val = []\ny_val = []\nfor notumor in val_notumor_path :\n    image = tf.keras.preprocessing.image.load_img(notumor,\n                                                  color_mode = 'grayscale')\n    image = np.array(image)\n    X_val.append(image)\n    y_val.append(0)\n    \nfor meningioma in val_meningioma_path :\n    image = tf.keras.preprocessing.image.load_img(meningioma,\n                                                  color_mode = 'grayscale')\n    image = np.array(image)\n    X_val.append(image)\n    y_val.append(1)\n    \nfor glioma in val_glioma_path :\n    image = tf.keras.preprocessing.image.load_img(glioma,\n                                                  color_mode = 'grayscale')\n    image = np.array(image)\n    X_val.append(image)\n    y_val.append(2)\n    \nfor pituitary in val_pituitary_path :\n    image = tf.keras.preprocessing.image.load_img(pituitary,\n                                                  color_mode = 'grayscale')\n    image = np.array(image)\n    X_val.append(image)\n    y_val.append(3)\n    \nX_val = np.array(X_val)\ny_val = np.array(y_val)\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#val_notumor[0]\nX_val = []\nX_val_name = []\ny_val = []\nfor notumor in val_notumor_path2 :\n    dir_ = './VAL/NO_TUMOR_FIX/' + notumor.split('_fix/', 1)[1]\n    image = tf.keras.preprocessing.image.load_img(dir_,\n                                                  color_mode = 'grayscale')\n    image = np.array(image)\n    X_val_name.append(dir_)\n    X_val.append(image)\n    y_val.append(0)\n    \nfor meningioma in val_meningioma_path2 :\n    dir_ = './VAL/MENINGIOMA_TUMOR_FIX/' + meningioma.split('_fix/', 1)[1]\n    image = tf.keras.preprocessing.image.load_img(dir_,\n                                                  color_mode = 'grayscale')\n    image = np.array(image)\n    X_val_name.append(dir_)\n    X_val.append(image)\n    y_val.append(1)\n    \nfor glioma in val_glioma_path2 :\n    dir_ = './VAL/GLIOMA_TUMOR_FIX/' + glioma.split('_fix/', 1)[1]\n    image = tf.keras.preprocessing.image.load_img(dir_,\n                                                  color_mode = 'grayscale')\n    image = np.array(image)\n    X_val_name.append(dir_)\n    X_val.append(image)\n    y_val.append(2)\n    \nfor pituitary in val_pituitary_path2 :\n    dir_ = './VAL/PITUITARY_TUMOR_FIX/' + pituitary.split('_fix/', 1)[1]\n    image = tf.keras.preprocessing.image.load_img(dir_,\n                                                  color_mode = 'grayscale')\n    image = np.array(image)\n    X_val_name.append(dir_)\n    X_val.append(image)\n    y_val.append(3)\n    \nX_val = np.array(X_val)\ny_val = np.array(y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var = train_pituitary_path\ncrop_var = crop_train_pituitary\ncrop_plot(np.array(var)[random_index], crop_var[random_index], 'Hipofisis')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cv2.IMREAD_GRAYSCALE(train_notumor_path[0])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train1, X_val1, y_train1, y_val1 = train_test_split(X_train,\n                                                      y_train,\n                                                      train_size = .8,\n                                                      random_state = 1)\n\nX_train2, X_val2, y_train2, y_val2 = train_test_split(X_train,\n                                                      y_train,\n                                                      train_size = .8,\n                                                      random_state = 2)\n\nX_train3, X_val3, y_train3, y_val3 = train_test_split(X_train,\n                                                      y_train,\n                                                      train_size = .8,\n                                                      random_state = 3)\n\nX_train4, X_val4, y_train4, y_val4 = train_test_split(X_train,\n                                                      y_train,\n                                                      train_size = .8,\n                                                      random_state = 4)\n\nX_train5, X_val5, y_train5, y_val5 = train_test_split(X_train,\n                                                      y_train,\n                                                      train_size = .8,\n                                                      random_state = 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def inception(x, filters):\n    # 1x1\n    path1 = Conv2D(filters=filters[0], kernel_size=(1,1), strides=1, padding='same', activation='relu')(x)\n\n    # 1x1->3x3\n    path2 = Conv2D(filters=filters[1][0], kernel_size=(1,1), strides=1, padding='same', activation='relu')(x)\n    path2 = Conv2D(filters=filters[1][1], kernel_size=(3,3), strides=1, padding='same', activation='relu')(path2)\n    \n    # 1x1->5x5\n    path3 = Conv2D(filters=filters[2][0], kernel_size=(1,1), strides=1, padding='same', activation='relu')(x)\n    path3 = Conv2D(filters=filters[2][1], kernel_size=(5,5), strides=1, padding='same', activation='relu')(path3)\n\n    # 3x3->1x1\n    path4 = MaxPooling2D(pool_size=(3,3), strides=1, padding='same')(x)\n    path4 = Conv2D(filters=filters[3], kernel_size=(1,1), strides=1, padding='same', activation='relu')(path4)\n\n    return Concatenate(axis=-1)([path1,path2,path3,path4])\n\n\ndef auxiliary(x, name=None):\n    layer = AveragePooling2D(pool_size=(5,5), strides=3, padding='valid')(x)\n    layer = Conv2D(filters=128, kernel_size=(1,1), strides=1, padding='same', activation='relu')(layer)\n    layer = Flatten()(layer)\n    layer = Dense(units=256, activation='relu',kernel_regularizer=regularizers.l2(0.0001))(layer)\n    layer = Dropout(0.5)(layer)\n    layer = Dense(units=CLASS_NUM, activation='softmax', name=name)(layer)\n    return layer\n\n\ndef googlenet():\n    layer_in = Input(shape=IMAGE_SHAPE)\n    \n    # start of stem mechanism\n    # stage-1\n    layer = Conv2D(filters=64, kernel_size=(7,7), strides=2, padding='same', activation='relu')(layer_in)\n    layer = MaxPooling2D(pool_size=(3,3), strides=2, padding='same')(layer)\n    layer = BatchNormalization()(layer)\n\n    # stage-2\n    layer = Conv2D(filters=64, kernel_size=(1,1), strides=1, padding='same', activation='relu')(layer)\n    layer = Conv2D(filters=192, kernel_size=(3,3), strides=1, padding='same', activation='relu')(layer)\n    layer = BatchNormalization()(layer)\n    layer = MaxPooling2D(pool_size=(3,3), strides=2, padding='same')(layer)\n    # end of stem mechanism\n    \n    # stage-3\n    layer = inception(layer, [ 64,  (96,128), (16,32), 32]) #3a\n    layer = inception(layer, [128, (128,192), (32,96), 64]) #3b\n    layer = MaxPooling2D(pool_size=(3,3), strides=2, padding='same')(layer)\n    \n    # stage-4\n    layer = inception(layer, [192,  (96,208),  (16,48),  64]) #4a\n    aux1  = auxiliary(layer, name='aux1')\n    layer = inception(layer, [160, (112,224),  (24,64),  64]) #4b\n    layer = inception(layer, [128, (128,256),  (24,64),  64]) #4c\n    layer = inception(layer, [112, (144,288),  (32,64),  64]) #4d\n    aux2  = auxiliary(layer, name='aux2')\n    layer = inception(layer, [256, (160,320), (32,128), 128]) #4e\n    layer = MaxPooling2D(pool_size=(3,3), strides=2, padding='same')(layer)\n    \n    # stage-5\n    layer = inception(layer, [256, (160,320), (32,128), 128]) #5a\n    layer = inception(layer, [384, (192,384), (48,128), 128]) #5b\n    layer = AveragePooling2D(pool_size=(7,7), strides=1, padding='valid')(layer)\n    \n    # stage-6\n    layer = Flatten()(layer)\n    layer = Dropout(0.5)(layer)\n    layer = Dense(units=256, activation='linear',kernel_regularizer=regularizers.l2(0.001))(layer)\n    main = Dense(units=CLASS_NUM, activation='softmax', name='main')(layer)\n    \n    model = Model(inputs=layer_in, outputs=[main, aux1, aux2])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CLASS_NUM = 4\nBATCH_SIZE = 8\nEPOCH_STEPS = int(X_train.shape[0]/BATCH_SIZE)\nIMAGE_SHAPE = (224, 224, 1)\nMODEL_NAME = 'skripsi_googlenet'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train model\nmodel = googlenet()\nmodel.summary()\n#model.load_weights(MODEL_NAME)\ntf.keras.utils.plot_model(model, 'GoogLeNet.png')\n\noptimizer = Adam(lr = 0.001, \n                 beta_1 = 0.9, \n                 beta_2 = 0.999, \n                 epsilon = 1e-08)\n#optimizer = SGD(lr=1 * 1e-1, momentum=0.9, nesterov=True)\n#model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\nmodel.compile(loss = \"sparse_categorical_crossentropy\", optimizer = optimizer, metrics = ['accuracy'])\n#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\noptimizer = ['Adam', 'SGD', 'Adam', 'SGD']\nepochs = [20, 30, 20, 30]\nhistory_all = {}\n\nhistory = model.fit(X_train,\n                    y_train,\n                    epochs = 50,\n                    steps_per_epoch = EPOCH_STEPS,\n                    validation_data = (X_val,y_val))\n\nmodel.save(MODEL_NAME)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = [15, 5])\nsns.set_style('whitegrid')\nplt.plot(history.history['main_accuracy'])\nplt.plot(history.history['val_main_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = [15, 5])\nplt.plot(history.history['main_loss'])\nplt.plot(history.history['val_main_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model.evaluate(X_val, y_val)\nprint('Score:', score[4])#akurasi\n\nprint('\\n loss')\nprint('Score:', score[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = prob_converter(model.predict(X_val))\nres_average = prob_average(model.predict(X_val))\nprint(confusion_matrix(np.array(res), np.array(y_val)))\nprint(accuracy_score(np.array(res), np.array(y_val)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train model\nCLASS_NUM = 4\nBATCH_SIZE = 8\nEPOCH_STEPS = int(X_train1.shape[0]/BATCH_SIZE)\nIMAGE_SHAPE = (224, 224, 1)\nMODEL_NAME = 'skripsi_googlenet'\n\n\nmodel = googlenet()\nmodel.summary()\n#model.load_weights(MODEL_NAME)\ntf.keras.utils.plot_model(model, 'GoogLeNet.png')\n\noptimizer = Adam(lr = 0.001, \n                 beta_1 = 0.9, \n                 beta_2 = 0.999, \n                 epsilon = 1e-08)\n#optimizer = SGD(lr=1 * 1e-1, momentum=0.9, nesterov=True)\n#model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\nmodel.compile(loss = \"sparse_categorical_crossentropy\", optimizer = optimizer, metrics = ['accuracy'])\n#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\noptimizer = ['Adam', 'SGD', 'Adam', 'SGD']\nepochs = [20, 30, 20, 30]\nhistory_all = {}\n\nhistory = model.fit(X_train1,\n                    y_train1,\n                    epochs = 50,\n                    steps_per_epoch = EPOCH_STEPS,\n                    validation_data = (X_val1,y_val1))\n\nmodel.save(MODEL_NAME)\n\nres = prob_converter(model.predict(X_val1))\nres_average = prob_average(model.predict(X_val1))\nprint(confusion_matrix(np.array(res), np.array(y_val1)))\nprint(accuracy_score(np.array(res), np.array(y_val1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train model\nCLASS_NUM = 4\nBATCH_SIZE = 8\nEPOCH_STEPS = int(X_train2.shape[0]/BATCH_SIZE)\nIMAGE_SHAPE = (224, 224, 1)\nMODEL_NAME = 'skripsi_googlenet'\n\n\nmodel = googlenet()\nmodel.summary()\n#model.load_weights(MODEL_NAME)\ntf.keras.utils.plot_model(model, 'GoogLeNet.png')\n\noptimizer = Adam(lr = 0.001, \n                 beta_1 = 0.9, \n                 beta_2 = 0.999, \n                 epsilon = 1e-08)\n#optimizer = SGD(lr=1 * 1e-1, momentum=0.9, nesterov=True)\n#model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\nmodel.compile(loss = \"sparse_categorical_crossentropy\", optimizer = optimizer, metrics = ['accuracy'])\n#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\noptimizer = ['Adam', 'SGD', 'Adam', 'SGD']\nepochs = [20, 30, 20, 30]\nhistory_all = {}\n\nhistory = model.fit(X_train2,\n                    y_train2,\n                    epochs = 50,\n                    steps_per_epoch = EPOCH_STEPS,\n                    validation_data = (X_val2,y_val2))\n\nmodel.save(MODEL_NAME)\n\nres2 = prob_converter(model.predict(X_val2))\nres_average2 = prob_average(model.predict(X_val2))\nprint(confusion_matrix(np.array(res2), np.array(y_val2)))\nprint(accuracy_score(np.array(res2), np.array(y_val2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train model\nCLASS_NUM = 4\nBATCH_SIZE = 8\nEPOCH_STEPS = int(X_train3.shape[0]/BATCH_SIZE)\nIMAGE_SHAPE = (224, 224, 1)\nMODEL_NAME = 'skripsi_googlenet'\n\n\n\nmodel = googlenet()\nmodel.summary()\n#model.load_weights(MODEL_NAME)\ntf.keras.utils.plot_model(model, 'GoogLeNet.png')\n\noptimizer = Adam(lr = 0.001, \n                 beta_1 = 0.9, \n                 beta_2 = 0.999, \n                 epsilon = 1e-08)\n#optimizer = SGD(lr=1 * 1e-1, momentum=0.9, nesterov=True)\n#model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\nmodel.compile(loss = \"sparse_categorical_crossentropy\", optimizer = optimizer, metrics = ['accuracy'])\n#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\noptimizer = ['Adam', 'SGD', 'Adam', 'SGD']\nepochs = [20, 30, 20, 30]\nhistory_all = {}\n\nhistory = model.fit(X_train3,\n                    y_train3,\n                    epochs = 50,\n                    steps_per_epoch = EPOCH_STEPS,\n                    validation_data = (X_val3,y_val3))\n\nmodel.save(MODEL_NAME)\n\nres3 = prob_converter(model.predict(X_val3))\nres_average3 = prob_average(model.predict(X_val3))\nprint(confusion_matrix(np.array(res3), np.array(y_val3)))\nprint(accuracy_score(np.array(res3), np.array(y_val3)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train model\nCLASS_NUM = 4\nBATCH_SIZE = 8\nEPOCH_STEPS = int(X_train4.shape[0]/BATCH_SIZE)\nIMAGE_SHAPE = (224, 224, 1)\nMODEL_NAME = 'skripsi_googlenet'\n\n\nmodel = googlenet()\nmodel.summary()\n#model.load_weights(MODEL_NAME)\ntf.keras.utils.plot_model(model, 'GoogLeNet.png')\n\noptimizer = Adam(lr = 0.001, \n                 beta_1 = 0.9, \n                 beta_2 = 0.999, \n                 epsilon = 1e-08)\n#optimizer = SGD(lr=1 * 1e-1, momentum=0.9, nesterov=True)\n#model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\nmodel.compile(loss = \"sparse_categorical_crossentropy\", optimizer = optimizer, metrics = ['accuracy'])\n#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\noptimizer = ['Adam', 'SGD', 'Adam', 'SGD']\nepochs = [20, 30, 20, 30]\nhistory_all = {}\n\nhistory = model.fit(X_train4,\n                    y_train4,\n                    epochs = 50,\n                    steps_per_epoch = EPOCH_STEPS,\n                    validation_data = (X_val4,y_val4))\n\nmodel.save(MODEL_NAME)\n\nres4 = prob_converter(model.predict(X_val4))\nres_average4 = prob_average(model.predict(X_val4))\nprint(confusion_matrix(np.array(res4), np.array(y_val4)))\nprint(accuracy_score(np.array(res4), np.array(y_val4)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train model\nCLASS_NUM = 4\nBATCH_SIZE = 8\nEPOCH_STEPS = int(X_train5.shape[0]/BATCH_SIZE)\nIMAGE_SHAPE = (224, 224, 1)\nMODEL_NAME = 'skripsi_googlenet'\n\n\n\nmodel = googlenet()\nmodel.summary()\n#model.load_weights(MODEL_NAME)\ntf.keras.utils.plot_model(model, 'GoogLeNet.png')\n\noptimizer = Adam(lr = 0.001, \n                 beta_1 = 0.9, \n                 beta_2 = 0.999, \n                 epsilon = 1e-08)\n#optimizer = SGD(lr=1 * 1e-1, momentum=0.9, nesterov=True)\n#model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\nmodel.compile(loss = \"sparse_categorical_crossentropy\", optimizer = optimizer, metrics = ['accuracy'])\n#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\noptimizer = ['Adam', 'SGD', 'Adam', 'SGD']\nepochs = [20, 30, 20, 30]\nhistory_all = {}\n\nhistory = model.fit(X_train5,\n                    y_train5,\n                    epochs = 50,\n                    steps_per_epoch = EPOCH_STEPS,\n                    validation_data = (X_val5,y_val5))\n\nmodel.save(MODEL_NAME)\n\nres5 = prob_converter(model.predict(X_val5))\nres_average5 = prob_average(model.predict(X_val5))\nprint(confusion_matrix(np.array(res5), np.array(y_val5)))\nprint(accuracy_score(np.array(res5), np.array(y_val5)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_result = pd.DataFrame({'actual' : y_val,\n                          #'pred' : res,\n                          #'prob' : [list(i) for i in res_average]})\n#df_result.to_csv('pred result 9 jan 2.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sample_plot(nrow, ncol, data, index) :\n    fig = plt.figure(figsize = [3*ncol, 5*nrow])\n    i = 1\n    for index_ in index :\n        ax = fig.add_subplot(nrow, ncol, i)\n        plt.imshow(data[index_],\n                   cmap = 'gray')\n        plt.xticks([])\n        plt.yticks([])\n        plt.text(i-1, -2, index_, size = 30)\n        i += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def image_to_vector_converter(data) : \n    dim = data[0].shape[0]\n    result = []\n    for img in tqdm(data) :\n        res = []\n        for i in list(range(dim)) :\n            res += list(img[i])\n        result.append(res)\n    return pd.DataFrame(np.array(result))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_df = image_to_vector_converter(X_train)\nX_val_df = image_to_vector_converter(X_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_df1 = image_to_vector_converter(X_train1)\nX_val_df1 = image_to_vector_converter(X_val1)\n\nX_train_df2 = image_to_vector_converter(X_train2)\nX_val_df2 = image_to_vector_converter(X_val2)\n\nX_train_df3 = image_to_vector_converter(X_train3)\nX_val_df3 = image_to_vector_converter(X_val3)\n\nX_train_df4 = image_to_vector_converter(X_train4)\nX_val_df4 = image_to_vector_converter(X_val4)\n\nX_train_df5 = image_to_vector_converter(X_train5)\nX_val_df5 = image_to_vector_converter(X_val5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifiers = [LogisticRegression(),\n               GaussianNB(),\n               SVC(),\n               RandomForestClassifier(random_state = 2),\n               DecisionTreeClassifier(random_state = 2)]\n\nacc_score = []\nvar_score = []\nalgos = []\nfor classifier in tqdm(classifiers) :\n    algos.append(type(classifier).__name__)\n    result = []\n    \n    # set 1\n    classifier.fit(X_train_df1,\n                   y_train1)\n    pred = classifier.predict(X_val_df1)\n    result.append(accuracy_score(np.array(pred), np.array(y_val1)))\n    \n    # set 2\n    classifier.fit(X_train_df2,\n                   y_train2)\n    pred = classifier.predict(X_val_df2)\n    result.append(accuracy_score(np.array(pred), np.array(y_val2)))\n    \n    # set 3\n    classifier.fit(X_train_df3,\n                   y_train3)\n    pred = classifier.predict(X_val_df3)\n    result.append(accuracy_score(np.array(pred), np.array(y_val3)))\n    \n    # set 4\n    classifier.fit(X_train_df4,\n                   y_train4)\n    pred = classifier.predict(X_val_df4)\n    result.append(accuracy_score(np.array(pred), np.array(y_val4)))\n    \n    # set 2\n    classifier.fit(X_train_df5,\n                   y_train5)\n    pred = classifier.predict(X_val_df5)\n    result.append(accuracy_score(np.array(pred), np.array(y_val5)))\n    \n    result = np.array(result)\n    \n    acc_score.append(result.mean())\n    var_score.append(result.var())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame({'method' : algos,\n              'accuracy' : acc_score,\n              'variance' : var_score})\n\nalgos.append('CNN & GoogLeNet')\nacc_score.append(np.array([accuracy_score(np.array(res), np.array(y_val1)),\n          accuracy_score(np.array(res2), np.array(y_val2)),\n          accuracy_score(np.array(res3), np.array(y_val3)),\n          accuracy_score(np.array(res4), np.array(y_val4)),\n          accuracy_score(np.array(res5), np.array(y_val5))]).mean())\n\nvar_score.append(np.array([accuracy_score(np.array(res), np.array(y_val1)),\n          accuracy_score(np.array(res2), np.array(y_val2)),\n          accuracy_score(np.array(res3), np.array(y_val3)),\n          accuracy_score(np.array(res4), np.array(y_val4)),\n          accuracy_score(np.array(res5), np.array(y_val5))]).var())\n\nclassifiers_df_cv = pd.DataFrame({'method' : algos,\n                                  'accuracy' : acc_score,\n                                  'variance' : var_score})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = [15, 8])\nsns.barplot(data = classifiers_df_cv.drop(3).sort_values(by = 'accuracy', ascending = False),\n            x = 'accuracy',\n            y = 'method',\n            palette = 'viridis')\n\nfor index, value in enumerate(classifiers_df_cv.drop(3).sort_values(by = 'accuracy', ascending = False)['accuracy']) :\n    plt.text(value, index, '%.3f'%value)\n    \nfor index, value in enumerate(classifiers_df_cv.drop(3).sort_values(by = 'accuracy', ascending = False)['variance']) :\n    plt.text(0, index, 'var : ' + '%.4f'%value)\nplt.title('Barplot of Cross Validation Performance', size = 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifiers = [LogisticRegression(),\n               GaussianNB(),\n               SVC(),\n               RandomForestClassifier(random_state = 2),\n               DecisionTreeClassifier(random_state = 2)]\nacc_score = []\nconf_list = []\nalgos = []\nfor classifier in tqdm(classifiers) :\n    model = classifier.fit(X_train_df,\n                           y_train)\n    pred = classifier.predict(X_val_df)\n    algos.append(type(classifier).__name__)\n    acc_score.append(accuracy_score(np.array(pred), np.array(y_val)))\n    conf_list.append(list(confusion_matrix(np.array(pred), np.array(y_val))))\nclassifiers_df = pd.DataFrame({'method' : algos,\n                               'accuracy' : acc_score,\n                               'conf_list' : conf_list})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifiers_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifiers = [LogisticRegression(),\n               GaussianNB(),\n               SVC(),\n               RandomForestClassifier(random_state = 2),\n               DecisionTreeClassifier(random_state = 2)]\n\ncv_accuracy = []\ncv_var = []\nfor classifier in tqdm(classifiers) :\n    model = classifier.fit(X_train_df,\n                           y_train)\n    #pred = classifier.predict(X_val_df)\n    #algos.append(type(classifier).__name__)\n    #acc_score.append(accuracy_score(np.array(pred), np.array(y_val)))\n    #conf_list.append(list(confusion_matrix(np.array(pred), np.array(y_val))))\n    cv = cross_val_score(model,\n                         X_train_df,\n                         y_train,\n                         scoring = 'accuracy',\n                         cv = KFold(5))\n    cv_accuracy.append(cv.mean())\n    cv_var.append(cv.var())\nclassifiers_df_cv = pd.DataFrame({'method' : algos,\n                                  'accuracy' : cv_accuracy,\n                                  'conf_list' : cv_var})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#classifiers_df_cv\nalgos.append('CNN & GoogLeNet')\ncv_accuracy.append(np.array([accuracy_score(np.array(res), np.array(y_val1)),\n          accuracy_score(np.array(res2), np.array(y_val2)),\n          accuracy_score(np.array(res3), np.array(y_val3)),\n          accuracy_score(np.array(res4), np.array(y_val4)),\n          accuracy_score(np.array(res5), np.array(y_val5))]).mean())\n\ncv_var.append(np.array([accuracy_score(np.array(res), np.array(y_val1)),\n          accuracy_score(np.array(res2), np.array(y_val2)),\n          accuracy_score(np.array(res3), np.array(y_val3)),\n          accuracy_score(np.array(res4), np.array(y_val4)),\n          accuracy_score(np.array(res5), np.array(y_val5))]).var())\n\nclassifiers_df_cv = pd.DataFrame({'method' : algos,\n                                  'accuracy' : cv_accuracy,\n                                  'variance' : cv_var})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nsns.set()\nplt.figure(figsize = [15, 8])\nsns.set_style('whitegrid')\nsns.barplot(data = classifiers_df_cv.drop(3).sort_values(by = 'accuracy', ascending = False),\n            x = 'accuracy',\n            y = 'method',\n            palette = 'viridis',\n            ci = 'sd')\n\n#labels = ['%.4f'%classifiers_df_cv.drop(3).sort_values(by = 'accuracy', ascending = False)['variance'][5],\n                     #'%.4f'%classifiers_df_cv.drop(3).sort_values(by = 'accuracy', ascending = False)['variance'][4],\n                     #'%.4f'%classifiers_df_cv.drop(3).sort_values(by = 'accuracy', ascending = False)['variance'][0],\n                     #'%.4f'%classifiers_df_cv.drop(3).sort_values(by = 'accuracy', ascending = False)['variance'][2],\n                     #'%.4f'%classifiers_df_cv.drop(3).sort_values(by = 'accuracy', ascending = False)['variance'][1]]\n#classifiers_df_cv.drop(3).sort_values(by = 'accuracy', ascending = False)['variance'][0]\n\n\nplt.title('Barplot of Cross Validation Result', size = 20)\n#bars = [r for r in ax.get_children() if type(r)==Rectangle]\n#colors = [c.get_facecolor() for c in bars[:-1]] # I think the last Rectangle is the background\nfor index, value in enumerate(classifiers_df_cv.drop(3).sort_values(by = 'accuracy', ascending = False)['accuracy']) :\n    plt.text(value, index, '%.2f'%value)\n\nfor index, value in enumerate(classifiers_df_cv.drop(3).sort_values(by = 'accuracy', ascending = False)['variance']) :\n    plt.text(0, index, 'var : ' + '%.4f'%value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifiers = [LogisticRegression(),\n               GaussianNB(),\n               SVC(),\n               RandomForestClassifier(random_state = 2),\n               DecisionTreeClassifier(random_state = 2)]\nalgos = []\naccuracy = []\nfor classifier in tqdm(classifiers) :\n    algos.append(type(classifier).__name__)\n    classifier.fit(X_train_df,\n                   y_train)\n    pred = classifier.predict(X_val_df)\n    accuracy.append(accuracy_score(np.array(pred), y_val))\nvis_df = pd.DataFrame({'method' : algos,\n                       'accuracy' : accuracy})\nvis_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#vis_df.append(pd.DataFrame(['CNN & GoogLeNet', 0.953917]), columns = ['method', 'accuracy'])\nvis_df.append(pd.DataFrame({'method' : 'CNN & GoogLeNet',\n                            'accuracy' : [0.953917]}))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = [15, 8])\nsns.barplot(data = vis_df.append(pd.DataFrame({'method' : 'CNN & GoogLeNet',\n                            'accuracy' : [0.953917]})).drop(3).sort_values(by = 'accuracy',\n                                                                           ascending = False),\n            x = 'accuracy',\n            y = 'method',\n            palette = 'viridis')\nfor index, value in enumerate(vis_df.append(pd.DataFrame({'method' : 'CNN & GoogLeNet',\n                            'accuracy' : [0.953917]})).drop(3).sort_values(by = 'accuracy',\n                                                                           ascending = False)['accuracy']) :\n    plt.text(value, index, '%.3f'%value)\nplt.title('Classifiers Comparison on Validation Set', size = 20)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}